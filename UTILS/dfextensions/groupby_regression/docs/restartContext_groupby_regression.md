# ğŸ§­ **Restart Context â€” GroupBy Regression Transition (v2.0)**

*Last updated: 2025-10-25*  
*Status: 85% complete â€“ transition to new package structure finished; benchmark + documentation remaining.*

---

## ğŸ§© **Project Summary**

The **GroupBy Regression refactoring** separates the **robust (legacy/production)** implementation and the **optimized (v2â€“v4)** implementations into a proper Python package under `O2DPG/UTILS/dfextensions/groupby_regression/`.

**Status:**
- âœ… All tests passing (41/41)
- âœ… Git history fully preserved via `git mv`
- âœ… Cross-validation and comparison benchmarks working
- ğŸŸ¡ Remaining: New optimized-only benchmark + unified documentation

---

## ğŸ“ **Current Directory Structure**
```
groupby_regression/
â”‚
â”œâ”€â”€ __init__.py                               # Package exports
â”‚
â”œâ”€â”€ groupby_regression.py                     # Robust / production implementation
â”œâ”€â”€ groupby_regression_optimized.py           # Fast v2/v3/v4 implementations
â”‚
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ groupby_regression.md                 # Robust internal documentation
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_groupby_regression.py            # Robust tests (14 tests)
â”‚   â”œâ”€â”€ test_groupby_regression_optimized.py  # Optimized tests (24 tests)
â”‚   â””â”€â”€ test_cross_validation.py              # Cross-validation robust â†” v2/v4 (3 tests)
â”‚
â”œâ”€â”€ benchmarks/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ bench_groupby_regression.py           # Robust benchmark (working)
â”‚   â”œâ”€â”€ bench_comparison.py                   # Robust â†” v2/v4 comparison (working)
â”‚   â”œâ”€â”€ bench_out/                            # Output directory
â”‚   â”‚   â”œâ”€â”€ benchmark_report.txt
â”‚   â”‚   â””â”€â”€ benchmark_results.json
â”‚   â””â”€â”€ (TODO) bench_groupby_regression_optimized.py  # â† NEW: To be created
â”‚
â””â”€â”€ benchmark_results/
    â”œâ”€â”€ latest/                               # Current comparison results
    â””â”€â”€ history.csv                           # Performance trend tracking
```

---

## âœ… **Completed Work**

### Phase 0 â€“ Safety Tag
- âœ… Tag `v1.0-pre-restructure` created (safe rollback point)
- âœ… All tests passing at tag (38 tests)

### Phase 1 â€“ Package Structure
- âœ… Created `groupby_regression/` directory with subdirectories
- âœ… Added `__init__.py` files (package, tests, benchmarks)

### Phase 2 â€“ File Migration
- âœ… All files moved via `git mv` (history preserved)
- âœ… History verified: `git log --follow` works correctly

### Phase 3 â€“ Import Updates
- âœ… All imports converted to relative (`..`)
- âœ… Package exports enabled in `__init__.py`
- âœ… All tests passing (41/41)

### Phase 4 â€“ Cross-Validation Tests
- âœ… `tests/test_cross_validation.py` created
- âœ… Verifies structural (v2) and numerical (v4) parity
- âœ… Fast execution (< 3s) suitable for CI

### Phase 5 â€“ Comparison Benchmark
- âœ… `benchmarks/bench_comparison.py` created
- âœ… Compares robust vs v2 vs v4
- âœ… Outputs: TXT report + CSV data + history tracking
- âœ… CI-friendly (â‰¤5 min in quick mode)

---

## ğŸ“Š **Performance Findings**

| Engine     | Speed [s/1k groups] | Speedup vs Robust |
|------------|---------------------|-------------------|
| Robust     | ~26 s/1k           | 1Ã— (baseline)     |
| v2 (loky)  | ~0.30 s/1k         | **â‰ˆ85Ã—**          |
| v4 (Numba) | ~0.0001 s/1k       | **â‰ˆ17,000Ã—**      |

**Key Insights:**
- âš ï¸ Robust implementation degrades significantly on small groups (< 50 rows/group)
- âœ… v2/v4 are numerically stable within 1e-7 on standard scenarios
- âš ï¸ Larger numerical differences (~0.57 max absolute slope difference) observed on small-group edge cases

---

## ğŸ“ **Import Changes (v2.0)**

### New Package Structure (v2.0+)
```python
# Correct imports for v2.0
from dfextensions.groupby_regression import GroupByRegressor
from dfextensions.groupby_regression import (
    make_parallel_fit_v2,
    make_parallel_fit_v3,
    make_parallel_fit_v4,
    GroupByRegressorOptimized,
)

# Example usage - Robust
_, dfGB = GroupByRegressor.make_parallel_fit(df, gb_columns=..., ...)

# Example usage - Fast
_, dfGB = make_parallel_fit_v4(df, gb_columns=..., ...)
```

### Old Paths (no longer work - breaking change)
```python
# âŒ These imports no longer work:
from dfextensions import GroupByRegressor  # FAILS
from dfextensions import make_parallel_fit_v4  # FAILS
```

---

## ğŸ”„ **Quick Reference Commands**
```bash
# Run all tests
cd ~/alicesw/O2DPG/UTILS/dfextensions/groupby_regression
pytest tests/ -v

# Run specific test suites
pytest tests/test_groupby_regression.py -v          # Robust (14 tests)
pytest tests/test_groupby_regression_optimized.py -v  # Optimized (24 tests)
pytest tests/test_cross_validation.py -v            # Cross-val (3 tests)

# Run benchmarks
python benchmarks/bench_comparison.py --scenarios quick
python benchmarks/bench_groupby_regression.py --quick

# Check git history preservation
git log --follow --oneline groupby_regression/groupby_regression.py | head -10

# Check current status
git status
git log --oneline -5
```

---

## ğŸ”§ **Remaining Work**

### 1ï¸âƒ£ **Create Optimized-Only Benchmark** ğŸ¯ NEXT TASK

**File:** `benchmarks/bench_groupby_regression_optimized.py`

**Purpose:** Benchmark v2/v3/v4 only â€“ omit slow robust implementation to enable large-scale tests.

**Requirements:**
- Use `benchmarks/bench_groupby_regression.py` as template
- Test engines: v2 (loky), v3 (threads), v4 (Numba JIT)
- Add JIT warm-up for v4 (exclude compilation from timing)
- Add environment stamp (capture versions/hardware)
- Support large-scale scenarios (up to 100k groups)
- CLI: `--quick` (â‰¤2k groups, <5min) and `--full` (â‰¤100k groups, <30min)
- Outputs: TXT report + JSON results + CSV summary

**CSV Schema (locked):**
```
date,host,commit,scenario,engine,n_groups,rows_per_group,
duration_s,per_1k_s,speedup,max_abs_delta_slope,max_abs_delta_intercept,notes
```

**Tolerances:**
- Default: â‰¤1e-7 (numerical precision only)
- Small-group exceptions: â‰¤1e-5 (Huber vs OLS differences)
- Apply to: slopes and intercepts of all fitted coefficients

**Benchmark Tiers:**
- **Tier-A (CI):** `--quick` mode, â‰¤2k groups, <5min
- **Tier-B (Manual):** `--full` mode, â‰¤100k groups, <30min

**Environment Stamp Template:**
```python
def get_environment_info():
    """Capture environment for benchmark reproducibility."""
    import sys, platform, os
    import numpy as np, pandas as pd, numba, joblib
    
    return {
        "python": sys.version.split()[0],
        "numpy": np.__version__,
        "pandas": pd.__version__,
        "numba": numba.__version__,
        "numba_threads": numba.config.NUMBA_DEFAULT_NUM_THREADS,
        "threading_layer": numba.threading_layer(),
        "joblib": joblib.__version__,
        "cpu": platform.processor(),
        "cpu_cores": os.cpu_count(),
        "os": platform.platform(),
    }
```

**JIT Warm-up Pattern:**
```python
def warm_up_numba():
    """Trigger Numba compilation before timing (call once at start)."""
    # Small dataset to trigger JIT compilation
    df_warmup = create_benchmark_data(10, 5, seed=999)
    gb_cols = ['xBin', 'y2xBin', 'z2xBin']
    sel = pd.Series(True, index=df_warmup.index)
    
    # Discard result - only purpose is to compile kernels
    _ = make_parallel_fit_v4(
        df=df_warmup,
        gb_columns=gb_cols,
        fit_columns=['dX'],
        linear_columns=['deltaIDC'],
        median_columns=[],
        weights='weight',
        suffix='_warmup',
        selection=sel,
        min_stat=3
    )
```

**Import Pattern (from bench_comparison.py):**
```python
# Handle imports for both direct execution and module import
try:
    # Try package-relative import first (when run as module)
    from ..groupby_regression_optimized import (
        make_parallel_fit_v2,
        make_parallel_fit_v3,
        make_parallel_fit_v4,
    )
except ImportError:
    # Fall back to adding parent to path (when run as script)
    script_dir = Path(__file__).parent
    package_dir = script_dir.parent
    sys.path.insert(0, str(package_dir))
    
    from groupby_regression_optimized import (
        make_parallel_fit_v2,
        make_parallel_fit_v3,
        make_parallel_fit_v4,
    )
```

---

### 2ï¸âƒ£ **Unified Documentation**

**File:** `docs/README.md`

**Sections needed:**
1. Quick Start (both implementations)
2. **Choosing Between Robust and Optimized** (critical guidance)
3. API Reference (both implementations)
4. Performance Benchmarks (how to run + interpret results)
5. Migration Guide (v1.0 â†’ v2.0 import changes)
6. Future Extensions (Sliding Window / Non-linear)

---

## ğŸ§  **Technical Decisions Made**

### Key Choices:
âœ… **No backward compatibility shims** (clean break)  
âœ… **Preserve git history** via `git mv`  
âœ… **Realistic tolerances** (1e-5 for implementation differences)  
âœ… **Two-tier benchmarking** (CI quick + manual full)  
âœ… **Both implementations maintained** (neither deprecated)  
âœ… **JIT warm-up excluded** from timing measurements  
âœ… **Environment stamping** in all benchmarks

### Known Issues (Deferred):
ğŸ“ **0.57 slope difference on small groups:**
- **Metric:** Max absolute difference in slope coefficients
- **Conditions:** 100 groups Ã— 5 rows/group, minimal noise
- **Expected:** <1e-7 (numerical precision)
- **Observed:** 0.57 (unexpectedly large)
- **Hypothesis:** Robust implementation may fail silently on very small groups
- **Status:** Investigation deferred until after restructuring complete

---

## ğŸ¯ **Implementation Status**

| Component | Status | Tests | Notes |
|-----------|--------|-------|-------|
| Package structure | âœ… Complete | - | All `__init__.py` files in place |
| File migration | âœ… Complete | - | History preserved with `git mv` |
| Import updates | âœ… Complete | 41/41 âœ… | All relative imports working |
| Cross-validation tests | âœ… Complete | 3/3 âœ… | Fast (<3s), always enabled |
| Comparison benchmark | âœ… Complete | - | Working, committed |
| Robust benchmark | âœ… Complete | - | Working, import fixed |
| **Optimized benchmark** | ğŸŸ¡ **In Progress** | - | **â† CURRENT TASK** |
| Documentation | ğŸŸ¡ Pending | - | Next after benchmark |

---

## ğŸ—“ï¸ **Next-Step Plan**

| Step | Owner | Duration | Status |
|------|-------|----------|--------|
| 1. Create `bench_groupby_regression_optimized.py` | GPT | â‰ˆ1h | ğŸŸ¡ **CURRENT** |
| 2. Test benchmark (`--quick` mode) | User | 30min | ğŸŸ¡ Pending |
| 3. Commit benchmark + results | User | 15min | ğŸŸ¡ Pending |
| 4. Write `docs/README.md` | Claude | 2-3h | ğŸŸ¡ Pending |
| 5. Final validation (all tests + benchmarks) | User | 1h | ğŸŸ¡ Pending |

---

## âœ… **Success Criteria**

- [x] All tests passing (41/41)
- [x] Package structure complete
- [x] Comparison benchmark working
- [x] Robust benchmark working
- [ ] **Optimized benchmark working** â† CURRENT GOAL
- [ ] **Documentation complete**
- [ ] Real TPC calibration data validated

---

## ğŸ“Œ **Notes for Implementation**

### Deprecation Policy
- **Robust:** Maintained, production-proven, NOT deprecated
- **Optimized:** Maintained, performance-optimized
- Both are supported, first-class implementations

### Future Extensions (Reserved Names)
```python
# Reserved for future versions (not yet implemented):
#   make_sliding_window_fit(...)  # Rolling window regression
#   make_nonlinear_fit(...)        # Non-linear models
```

### Test Discovery
A `pytest.ini` file may be added:
```ini
[pytest]
testpaths = groupby_regression/tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*
addopts = -v --tb=short
```

---

---

# ğŸš€ **RESTART PROMPT FOR GPT**

*Use this section when restarting a GPT session to create the optimized benchmark.*

---

## Project Restart: GroupBy Regression Optimized Benchmark

### ğŸ“ Attached Files
- `restartContext.md` - This file (complete project context)
- `benchmarks/bench_groupby_regression.py` - Template to adapt
- `benchmarks/bench_comparison.py` - Import pattern reference
- `groupby_regression_optimized.py` - v2/v3/v4 implementations to benchmark
- `__init__.py` - Package structure reference

### ğŸ¯ Immediate Goal
Create `benchmarks/bench_groupby_regression_optimized.py` - a comprehensive benchmark script for v2/v3/v4 engines only (omit slow robust implementation to enable large-scale tests up to 100k groups).

### ğŸ“š Context Loading Instructions

**Please follow these steps IN ORDER:**

#### Step 1: Read and Absorb Context
Read all sections of `restartContext.md` above, especially:
- "Remaining Work" section (your task details)
- "Performance Findings" (speedup data)
- "Technical Decisions Made" (constraints)
- Code templates (environment stamp, JIT warm-up, import pattern)

#### Step 2: Demonstrate Understanding
Before writing any code, confirm you understand:

1. **Task:** Create what file, for what purpose?
2. **Template:** Which existing file should you adapt?
3. **Engines:** Which implementations to test (v2/v3/v4)?
4. **Key additions:** What must you add beyond the template (JIT warm-up, environment stamp)?
5. **Output format:** What files should the benchmark produce?
6. **CLI tiers:** What's the difference between `--quick` and `--full` modes?

**Respond with:** A brief summary (2-3 sentences) showing you understand the task.

#### Step 3: Ask Clarifying Questions
If ANYTHING is unclear or ambiguous, ask questions NOW. Examples of good questions:
- "Should v3 use the same scenarios as v2, or different ones?"
- "In the warm-up function, should I test all three engines or just v4?"
- "Should I match the exact CLI arguments of bench_groupby_regression.py?"
- "What should `--rows-per-group` default to in --full mode?"

**If everything is clear, say:** "All clear, ready to proceed."

#### Step 4: Propose Implementation Approach
Briefly outline your plan:
- File structure (functions, classes, main flow)
- How you'll integrate warm-up and environment stamping
- How you'll handle the three engines (v2, v3, v4)
- CLI argument design

**Wait for confirmation** before coding.

#### Step 5: Implementation
Only after Steps 1-4 complete and confirmed, provide the complete, runnable script.

### ğŸš« What NOT to Do
- âŒ Don't jump straight to code
- âŒ Don't make assumptions about unclear requirements
- âŒ Don't provide partial implementations without asking
- âŒ Don't skip the warm-up or environment stamping

### âœ… Success Criteria
A Python script that:
- âœ… Runs successfully: `python bench_groupby_regression_optimized.py --quick`
- âœ… Tests all three engines: v2, v3, v4
- âœ… Completes in <5 minutes (quick mode)
- âœ… Outputs TXT report + JSON results + (optional) CSV summary
- âœ… Includes JIT warm-up for v4
- âœ… Includes environment info in output
- âœ… Uses correct import pattern (try/except from bench_comparison.py)

### ğŸ“ Additional Context
**You recently passed a self-check demonstrating you understand:**
- Difference between v2 (loky), v3 (threads), v4 (Numba JIT)
- Why v4 needs warm-up (compilation time would distort results)
- Speedup numbers (v4 is ~17,000Ã— faster than robust)

**This knowledge is correct - use it in your implementation.**

---

**Ready? Start with Step 2: Demonstrate your understanding of the task.**
