{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e25edcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This makes a smaller AO2D to work with (just one DF)\n",
    "import ROOT\n",
    "\n",
    "# File paths\n",
    "input_file = \"AO2D_o2_ctf_run00544742_orbit0212902048_tf0004599497_epn309.root\"\n",
    "\n",
    "# Open the input ROOT file\n",
    "infile = ROOT.TFile.Open(input_file, \"READ\")\n",
    "\n",
    "# Find the first TDirectory starting with \"DF_\"\n",
    "df_dir = None\n",
    "dir_name = \"\"\n",
    "for key in infile.GetListOfKeys():\n",
    "    name = key.GetName()\n",
    "    if name.startswith(\"DF_\"):\n",
    "        # Access the TDirectory\n",
    "        df_dir = infile.Get(name)\n",
    "        dir_name = name\n",
    "        break\n",
    "\n",
    "if not df_dir:\n",
    "    raise RuntimeError(\"No TDirectory starting with 'DF_' found.\")\n",
    "\n",
    "# Open the output file (create if not exist)\n",
    "output_file = \"AO2D_reduced_\" + str(dir_name) + \".root\"\n",
    "outfile = ROOT.TFile.Open(output_file, \"RECREATE\")\n",
    "\n",
    "# Create the same directory structure in the output file\n",
    "df_dir_copy = outfile.mkdir(dir_name)\n",
    "\n",
    "# Move to the newly created directory\n",
    "df_dir_copy.cd()\n",
    "\n",
    "# Loop over the keys (trees) inside the \"DF_\" directory and copy them\n",
    "for key in df_dir.GetListOfKeys():\n",
    "    obj = df_dir.Get(key.GetName())\n",
    "    if isinstance(obj, ROOT.TTree):  # Check if it's a TTree\n",
    "        # Clone the tree and write it to the corresponding directory in the output file\n",
    "        obj.CloneTree(-1).Write(key.GetName(), ROOT.TObject.kOverwrite)  # Copy the tree\n",
    "\n",
    "# Now handle the metaData;1 key (TMap) in the top-level directory\n",
    "meta_data = infile.Get(\"metaData\")\n",
    "if meta_data:\n",
    "    if isinstance(meta_data, ROOT.TMap):\n",
    "       copied_meta_data = meta_data.Clone()\n",
    "       outfile.cd()  # Make sure we're at the top-level in the output file\n",
    "       outfile.WriteObject(meta_data, \"metaData\")\n",
    "\n",
    "       # Iterate over the map\n",
    "       iter = meta_data.MakeIterator()\n",
    "       entry = iter.Next()\n",
    "       while entry:\n",
    "         key = entry\n",
    "         value = meta_data.GetValue(key)\n",
    "\n",
    "         # Convert TObjString to Python string\n",
    "         key_str = key.GetName()\n",
    "         value_str = value.GetName() if value else \"None\"\n",
    "         print(f\"{key_str}: {value_str}\")\n",
    "         entry = iter.Next()\n",
    "\n",
    "        \n",
    "# Close the files\n",
    "outfile.Close()\n",
    "infile.Close()\n",
    "\n",
    "print(f\"Copied all trees from TDirectory '{dir_name}' to '{output_file}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f1d16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "LHCMaxBunches = 3564;                           # max N bunches\n",
    "LHCRFFreq = 400.789e6;                          # LHC RF frequency in Hz\n",
    "LHCBunchSpacingNS = 10 * 1.e9 / LHCRFFreq;      # bunch spacing in ns (10 RFbuckets)\n",
    "LHCOrbitNS = LHCMaxBunches * LHCBunchSpacingNS; # orbit duration in ns\n",
    "LHCRevFreq = 1.e9 / LHCOrbitNS;                 # revolution frequency\n",
    "LHCBunchSpacingMUS = LHCBunchSpacingNS * 1e-3;  # bunch spacing in \\mus (10 RFbuckets)\n",
    "LHCOrbitMUS = LHCOrbitNS * 1e-3;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac88c44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ROOT import o2\n",
    "\n",
    "run_number = 544742\n",
    "\n",
    "def retrieve_Aggregated_RunInfos(run_number):\n",
    "    runInfo = o2.parameters.AggregatedRunInfo.buildAggregatedRunInfo(o2.ccdb.BasicCCDBManager.instance(), run_number)\n",
    "    detList = o2.detectors.DetID.getNames(runInfo.grpECS.getDetsReadOut())\n",
    "    assert (run_number == runInfo.runNumber)\n",
    "    assert (run_number == runInfo.grpECS.getRun())\n",
    "    return {\"SOR\" : runInfo.sor,\n",
    "            \"EOR\" : runInfo.eor,\n",
    "            \"FirstOrbit\" : runInfo.orbitSOR,\n",
    "            \"LastOrbit\" : runInfo.orbitEOR,\n",
    "            \"OrbitReset\" : runInfo.orbitReset,\n",
    "            \"OrbitsPerTF\" : int(runInfo.orbitsPerTF),\n",
    "            \"detList\" : detList}\n",
    "\n",
    "run_info = retrieve_Aggregated_RunInfos(run_number)\n",
    "\n",
    "# update num of timeframes\n",
    "# figure out how many timeframes fit into this run range\n",
    "# take the number of orbits per timeframe and multiply by orbit duration to calculate how many timeframes fit into this run\n",
    "time_length_inmus = 1000 * (run_info[\"EOR\"] - run_info[\"SOR\"])\n",
    "ntimeframes = time_length_inmus / (run_info[\"OrbitsPerTF\"] * LHCOrbitMUS)\n",
    "# also calculate how many orbits fit into the run range\n",
    "print (f\"This run has space for {ntimeframes} timeframes\")\n",
    "    \n",
    "run_info[\"ntimeframes\"] = ntimeframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94ac387",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80982aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bc_with_timestamps(bc_data, run_info):\n",
    "  \"\"\"\n",
    "  bc_data is a pandas df containing the AO2D basic bunch crossing data\n",
    "  \"\"\"\n",
    "  \n",
    "  # add a new column to the bc table dynamically\n",
    "  # this is the time in mu s\n",
    "  bc_data[\"timestamp\"] = run_info[\"OrbitReset\"] + (bc_data[\"fGlobalBC\"] * LHCBunchSpacingMUS).astype(\"int64\")\n",
    "  bc_data[\"timeframeID\"] = ((bc_data[\"fGlobalBC\"] - (run_info[\"FirstOrbit\"] * LHCMaxBunches)) / (LHCMaxBunches * run_info[\"OrbitsPerTF\"])).astype(\"int64\")\n",
    "  bc_data[\"orbit\"] = (bc_data[\"fGlobalBC\"] // LHCMaxBunches).astype(\"int64\")\n",
    "  bc_data[\"bc_within_orbit\"] = (bc_data[\"fGlobalBC\"] % LHCMaxBunches).astype(\"int64\")\n",
    "  return bc_data\n",
    "\n",
    "\n",
    "def get_timeframe_structure(filepath, run_info, max_folders=1, include_dataframe = False, folder_filter=None):\n",
    "  \"\"\"\n",
    "  run_info: The aggregated run_info object for this run\n",
    "  \"\"\"\n",
    "  def find_tree_key(keys, pattern):\n",
    "     for key in keys:\n",
    "        key_clean = key\n",
    "        if re.search(pattern, key_clean, re.IGNORECASE):\n",
    "            return key_clean\n",
    "     return None\n",
    "\n",
    "  file = uproot.open(filepath)\n",
    "  raw_keys = file.keys()\n",
    "  #print (raw_keys)\n",
    "    \n",
    "  folders = { k.split(\"/\")[0] : 1 for k in raw_keys if \"O2bc_001\" in k }\n",
    "  #print (folders)\n",
    "  folders = [ k for k in folders.keys() ] \n",
    "  #print (folders)\n",
    "  folders = folders[:max_folders]  \n",
    "\n",
    "  print (\"have \", len(raw_keys), f\" in file {filepath}\")\n",
    "  # print (folders)\n",
    "\n",
    "  merged = {} # data containers per file\n",
    "  for folder in folders:\n",
    "    if folder_filter != None and folder != folder_filter:\n",
    "        continue\n",
    "    #print (f\"Looking into {folder}\")\n",
    "    \n",
    "    # Find correct table names using regex\n",
    "    bc_key = find_tree_key(raw_keys, f\"^{folder}/O2bc_001\")\n",
    "    bc_data = file[bc_key].arrays(library=\"pd\")\n",
    "\n",
    "    # collision data\n",
    "    coll_key = find_tree_key(raw_keys, f\"^{folder}/O2coll.*_001\")\n",
    "    coll_data = file[coll_key].arrays(library=\"pd\")\n",
    "    \n",
    "    # extend the data\n",
    "    bc_data = get_bc_with_timestamps(bc_data, run_info)\n",
    "    \n",
    "    # do the splice with collision data\n",
    "    bc_data_coll = bc_data.iloc[coll_data[\"fIndexBCs\"]].reset_index(drop=True)\n",
    "    # this is the combined table containing collision data associated to bc and time information\n",
    "    combined = pd.concat([bc_data_coll, coll_data], axis = 1)\n",
    "    \n",
    "    # do the actual timeframe structure calculation; we only take collisions with a trigger decision attached\n",
    "    triggered = combined[combined[\"fTriggerMask\"] != 0]\n",
    "    timeframe_structure = triggered.groupby('timeframeID').apply(\n",
    "      lambda g: list(zip(g['fGlobalBC'], g['fPosX'], g['fPosY'], g['fPosZ'], g['orbit'], g['bc_within_orbit'], g['fCollisionTime']))\n",
    "    ).reset_index(name='position_vectors')\n",
    "    \n",
    "    folderkey = folder + '@' + filepath\n",
    "    merged[folderkey] = timeframe_structure # data per folder\n",
    "    if include_dataframe:\n",
    "        merged[\"data\"] = combined\n",
    "  \n",
    "  # annotate which timeframes are available here and from which file\n",
    "\n",
    "  return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1733bb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged=get_timeframe_structure('AO2D_o2_ctf_run00544742_orbit0212902048_tf0004599497_epn309.root', run_info, max_folders=1000)\n",
    "\n",
    "# Iterating over rows\n",
    "for key in merged:\n",
    "  result = merged[key]\n",
    "  print (key)\n",
    "  for index, row in result.iterrows():\n",
    "    tf = row['timeframeID']\n",
    "    num_cols = len(row['position_vectors'])\n",
    "    print (f\"Timeframe {tf} has {num_cols} collisions\")\n",
    "    orbit = row['position_vectors'][0][4]\n",
    "    bc = row['position_vectors'][0][0]\n",
    "    bc_relative = row['position_vectors'][0][5]\n",
    "    col_time = row['position_vectors'][0][6]\n",
    "    print (f\"Global BC {bc}; orbit {orbit}; bc_relative {bc_relative}; col time {col_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0420319a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_bccoll_to_localFile(alien_file, local_filename):\n",
    "\n",
    "  # Open the remote file via AliEn\n",
    "  infile = ROOT.TFile.Open(alien_file, \"READ\")\n",
    "  if not infile or infile.IsZombie():\n",
    "    raise RuntimeError(f\"Failed to open {alien_file}\")\n",
    "\n",
    "  # Output local file\n",
    "  outfile = ROOT.TFile.Open(local_filename, \"RECREATE\")\n",
    "\n",
    "  # List of trees to copy\n",
    "  trees_to_copy = [\"O2bc_001\", \"O2collision_001\"]\n",
    "\n",
    "  # Loop over top-level keys to find DF_ folders\n",
    "  for key in infile.GetListOfKeys():\n",
    "    obj = key.ReadObj()\n",
    "    if obj.InheritsFrom(\"TDirectory\") and key.GetName().startswith(\"DF_\"):\n",
    "        df_name = key.GetName()\n",
    "        df_dir = infile.Get(df_name)\n",
    "        \n",
    "        # Create corresponding folder in output file\n",
    "        out_df_dir = outfile.mkdir(df_name)\n",
    "        out_df_dir.cd()\n",
    "\n",
    "        # Copy only specified trees if they exist\n",
    "        for tree_name in trees_to_copy:\n",
    "            if df_dir.GetListOfKeys().FindObject(tree_name):\n",
    "                tree = df_dir.Get(tree_name)\n",
    "                cloned_tree = tree.CloneTree(-1)  # copy all entries\n",
    "                cloned_tree.Write(tree_name)\n",
    "\n",
    "        outfile.cd()  # go back to top-level for next DF_\n",
    "\n",
    "  # Close files\n",
    "  outfile.Close()\n",
    "  infile.Close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494b39ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ROOT\n",
    "# Connect to AliEn grid\n",
    "ROOT.TGrid.Connect(\"alien://\")\n",
    "\n",
    "#alien_file='alien:///alice/data/2023/LHC23zzm/544742/apass5/2350/o2_ctf_run00544742_orbit0137223104_tf0002234530_epn138/009/AO2D.root'\n",
    "#local_filename='AO2D_skimmed.root'\n",
    "#fetch_bccoll_to_localFile(alien_file, local_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c928423e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist = ['alien:///alice/data/2023/LHC23zzm/544742/apass5/0000/o2_ctf_run00544742_orbit0137377824_tf0002239365_epn262/001/AO2D.root',\n",
    "'alien:///alice/data/2023/LHC23zzm/544742/apass5/0000/o2_ctf_run00544742_orbit0137377824_tf0002239365_epn262/002/AO2D.root',\n",
    "'alien:///alice/data/2023/LHC23zzm/544742/apass5/0000/o2_ctf_run00544742_orbit0137377824_tf0002239365_epn262/003/AO2D.root',\n",
    "'alien:///alice/data/2023/LHC23zzm/544742/apass5/0000/o2_ctf_run00544742_orbit0137377824_tf0002239365_epn262/004/AO2D.root',\n",
    "'alien:///alice/data/2023/LHC23zzm/544742/apass5/0000/o2_ctf_run00544742_orbit0137377824_tf0002239365_epn262/005/AO2D.root',\n",
    "'alien:///alice/data/2023/LHC23zzm/544742/apass5/0000/o2_ctf_run00544742_orbit0137377824_tf0002239365_epn262/006/AO2D.root',\n",
    "'alien:///alice/data/2023/LHC23zzm/544742/apass5/0000/o2_ctf_run00544742_orbit0137377824_tf0002239365_epn262/007/AO2D.root',\n",
    "'alien:///alice/data/2023/LHC23zzm/544742/apass5/0000/o2_ctf_run00544742_orbit0137377824_tf0002239365_epn262/008/AO2D.root',\n",
    "'alien:///alice/data/2023/LHC23zzm/544742/apass5/0000/o2_ctf_run00544742_orbit0137377824_tf0002239365_epn262/009/AO2D.root',\n",
    "'alien:///alice/data/2023/LHC23zzm/544742/apass5/0000/o2_ctf_run00544742_orbit0137377824_tf0002239365_epn262/010/AO2D.root']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f5aca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hashlib\n",
    "timeframe_data = []\n",
    "for file in filelist[:4]:\n",
    "    # calculate filename hash\n",
    "    hash_name = hashlib.sha1(file.encode()).hexdigest()\n",
    "    local_filename = os.path.join(\"./\", f\"AO2D_{hash_name}.root\")\n",
    "    print (f\"Fetching data from {file} to local file {local_filename}\")\n",
    "    if not os.path.exists(local_filename):\n",
    "      fetch_bccoll_to_localFile(file, local_filename)\n",
    "    else:\n",
    "      print (\"Using file from local cache\")\n",
    "    \n",
    "    print (f\"Analyzing local file\")\n",
    "    merged=get_timeframe_structure(local_filename, run_info, max_folders=1000)\n",
    "    print (\"Got \" + str(len(merged)) + \" datasets\")\n",
    "    timeframe_data.append(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adec7583",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(timeframe_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b75c85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print stats and create reverse index from timeframe to DF_ and file\n",
    "timeframe_to_file={}\n",
    "for d in timeframe_data:\n",
    "  for key in d:\n",
    "    result = d[key]\n",
    "    print (key)\n",
    "    for index, row in result.iterrows():\n",
    "      tf = row['timeframeID']\n",
    "      num_cols = len(row['position_vectors'])\n",
    "      print (f\"Timeframe {tf} has {num_cols} collisions\")\n",
    "      timeframe_to_file[tf] = key\n",
    "        \n",
    "print (timeframe_to_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00f3abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ROOT import o2\n",
    "LHCMaxBunches = 3564\n",
    "def convert_to_digicontext(aod_timeframe=None, timeframeID=-1):\n",
    "    \"\"\"\n",
    "    converts AOD collision information from AO2D to collision context\n",
    "    which can be used for MC\n",
    "    \"\"\"\n",
    "    # we create the digitization context object\n",
    "    digicontext=o2.steer.DigitizationContext()\n",
    "    \n",
    "    # we can fill this container\n",
    "    parts = digicontext.getEventParts()\n",
    "    # we can fill this container\n",
    "    records = digicontext.getEventRecords()\n",
    "    # copy over information\n",
    "    maxParts = 1\n",
    "    \n",
    "    entry = 0\n",
    "    vertices = ROOT.std.vector(\"o2::math_utils::Point3D<float>\")()\n",
    "    vertices.resize(len(aod_timeframe))\n",
    "    \n",
    "    colindex = 0\n",
    "    for colindex, col in enumerate(aod_timeframe):\n",
    "        # we make an event interaction record\n",
    "        pvector = ROOT.std.vector(\"o2::steer::EventPart\")()\n",
    "        pvector.push_back(o2.steer.EventPart(0, colindex))\n",
    "        parts.push_back(pvector)\n",
    "        \n",
    "        orbit = col[4]\n",
    "        bc_within_orbit = col[5]\n",
    "        interaction_rec = o2.InteractionRecord(bc_within_orbit, orbit)\n",
    "        col_time_relative_to_bc = col[6] # in NS\n",
    "        time_interaction_rec = o2.InteractionTimeRecord(interaction_rec, col_time_relative_to_bc)\n",
    "        records.push_back(time_interaction_rec)\n",
    "        vertices[colindex].SetX(col[1])\n",
    "        vertices[colindex].SetY(col[2])\n",
    "        vertices[colindex].SetZ(col[3])\n",
    "        \n",
    "    digicontext.setInteractionVertices(vertices)\n",
    "    digicontext.setNCollisions(vertices.size())\n",
    "    digicontext.setMaxNumberParts(maxParts)\n",
    "    \n",
    "    # set the bunch filling ---> NEED to fetch it from CCDB\n",
    "    # digicontext.setBunchFilling(bunchFillings[0]);\n",
    "    \n",
    "    prefixes = ROOT.std.vector(\"std::string\")();\n",
    "    prefixes.push_back(\"sgn\")\n",
    "    \n",
    "    digicontext.setSimPrefixes(prefixes);\n",
    "    digicontext.printCollisionSummary();\n",
    "    digicontext.saveToFile(f\"collission_context_{timeframeID}.root\")\n",
    "    \n",
    "\n",
    "    \n",
    "convert_to_digicontext(timeframe_data[0]['DF_2339649212135456'].iloc[0]['position_vectors'], timeframe_data[0]['DF_2339649212135456'].iloc[0]['timeframeID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dd82e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeframe_data[0]['DF_2339649212135456'].iloc[0]['position_vectors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4dc5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in timeframe_data:\n",
    "  for key in d:\n",
    "    result = d[key]\n",
    "    for index, row in result.iterrows():\n",
    "      tf = row['timeframeID']\n",
    "      cols = row['position_vectors']\n",
    "      convert_to_digicontext(cols, tf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb904f8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be9ea69",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (run_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1bc8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# define which TIMEFRAME ID to simulate\n",
    "os.environ[\"ANCHOR_TIMEFRAME_ID\"] = \"2239364\"\n",
    "os.environ[\"ANCHOR_RUN_NUMBER\"] = \"544742\"\n",
    "os.environ[\"ANCHOR_FIRST_ORBIT\"] = \"65718176\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede73291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce a MC workflow for the given timestamp and conditions ----> This will produce an MC dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4efee23",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# open MC AO2D and contrast to DATA AO2D\n",
    "export O2DPG_ROOT=/home/swenzel/alisw/O2DPG\n",
    "pwd\n",
    "if [ ! -d \"MC_${ANCHOR_TIMEFRAME_ID}\" ]; then\n",
    "  mkdir MC_${ANCHOR_TIMEFRAME_ID}\n",
    "fi\n",
    "cd MC_${ANCHOR_TIMEFRAME_ID}\n",
    "pwd\n",
    "\n",
    "SIMENGINE=TGeant4\n",
    "NWORKERS=8\n",
    "${O2DPG_ROOT}/MC/bin/o2dpg_sim_workflow.py -eCM 14000  -col pp -gen pythia8 -proc cdiff -tf 1     \\\n",
    "                                                       -ns 2000 -e ${SIMENGINE}                   \\\n",
    "                                                       -j ${NWORKERS} -interactionRate 500000     \\\n",
    "                                                       -run ${ANCHOR_RUN_NUMBER} -seed 624        \\\n",
    "                                                       -productionTag \"alibi_O2DPG_pp_minbias\"    \\\n",
    "                                                       --production-offset ${ANCHOR_TIMEFRAME_ID} \\\n",
    "                                                       --data-anchoring ../                       \\\n",
    "                                                       --first-orbit ${ANCHOR_FIRST_ORBIT}\n",
    "\n",
    "${O2DPG_ROOT}/MC/bin/o2_dpg_workflow_runner.py -f workflow.json -tt aod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac63a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "MC_AOD_FILE='MC_2239364/AO2D.root'\n",
    "mc_data=get_timeframe_structure(MC_AOD_FILE, run_info, max_folders=1, include_dataframe=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df5680a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91d4bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeframe_to_file[2239364]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b44340b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_AOD='AO2D_a93f9e01edd28f0253aecfe2c40bebf0ecc2358a.root'\n",
    "data_data=get_timeframe_structure(data_AOD, run_info, max_folders=1, include_dataframe=True, folder_filter='DF_2339649212135456')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70d231b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df=data_data['data'][data_data['data']['timeframeID']==2239364]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf62fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedb90ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_df = mc_data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed336733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter\n",
    "# Step 1: Find intersection of X values\n",
    "common_bc = set(mc_df['fGlobalBC']).intersection(data_df['fGlobalBC'])\n",
    "len(common_bc)\n",
    "\n",
    "# Step 2: Filter both DataFrames\n",
    "df1_filtered = mc_df[mc_df['fGlobalBC'].isin(common_bc)]\n",
    "df2_filtered = data_df[data_df['fGlobalBC'].isin(common_bc)]\n",
    "len(df1_filtered)\n",
    "len(df2_filtered)\n",
    "# Step 3: Rename columns in df2 (except 'X')\n",
    "df2_renamed = df2_filtered.rename(columns={col: f\"{col}_data\" for col in df2_filtered.columns if col != 'fGlobalBC'})\n",
    "#result = pd.merge(df1_filtered, df2_renamed, on='fGlobalBC', how='inner')\n",
    "print (df2_renamed)\n",
    "# Step 3: Concatenate the filtered DataFrames\n",
    "# embedded_events = df1_filtered.join(df2_renamed).reset_index()\n",
    "#embedded_events = pd.concat([df1_filtered, df2_filtered], ignore_index=True)\n",
    "\n",
    "#df2_renamed = df2_filtered.rename(columns={col: f\"{col}_df2\" for col in df2_filtered.columns if col != 'X'})\n",
    "\n",
    "# Step 4: Merge the two DataFrames on 'X'\n",
    "embedded_events = pd.merge(df1_filtered, df2_renamed, on='fGlobalBC', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b3d8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (embedded_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6160d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_events[['fGlobalBC','orbit','bc_within_orbit','fPosX','fPosX_data', 'fPosY','fPosY_data','fPosZ','fPosZ_data' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cad84b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
